# Federated Learning with Gradient Compression

This is partly the reproduction of the paper of [To Talk or to Work: Flexible Communication Compression for Energy Efficient Federated Learning over Heterogeneous Mobile Edge Devices](IEEE Infocom 2021 paper)   
The experiments can be executed on ResNet-20 with CIFAR-10 and MNIST with LetNet-5.
We establish this framework based on shaoxiongji's work (https://github.com/shaoxiongji/federated-learning), where we mainly add the compression and GPU computing parts.

## Requirements
python>=3.6  
pytorch>=0.4


## References

Liang Li, Dian Shi, Ronghui Hou, Hui Li, Miao Pan, and Zhu Han, "To Talk or to Work: Flexible Communication Compression for Energy Efficient Federated Learning over Heterogeneous Mobile Edge Devices", IEEE International Conference on Computer Communications (INFOCOM'21), Virtual Conference, May 10-13, 2021
